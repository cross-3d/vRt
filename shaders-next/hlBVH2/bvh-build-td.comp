#version 460 core
#extension GL_GOOGLE_include_directive : enable

#ifdef FIRST_STEP
    #define WORK_SIZE_BND 64
#else
    #define WORK_SIZE_BND 1024
#endif

#define BVH_BUILD
#define BVH_CREATION
#define USE_ACTIVE

#include "../include/driver.glsl"
#include "../include/mathlib.glsl"
#include "../include/structs.glsl"
#include "../include/vertex.glsl"
#include "../include/ballotlib.glsl"
#include "./submodules/includes.glsl"

// used sort of https://research.nvidia.com/sites/default/files/pubs/2011-08_Simpler-and-Faster/main.pdf
// also, using better BVH2 layout


// 0 and 4 reserved counters
// shared memory counters
shared int _counters[8];
#define cBuffer _counters[3]

// define function for increment
initAtomicSubgroupIncFunctionTarget(_counters[WHERE], _aCounterInc, 1, int)
initAtomicSubgroupIncFunctionTarget(vtCounters[WHERE], vtCountersInc, 1, int)
initAtomicSubgroupIncFunctionTarget(vtCounters[WHERE], vtCountersIncBoth, 2, int)
int lCounterInc() {return vtCountersInc(0)*2;};

// shared memory counters
int cCounterInc() {return vtCountersInc(1);};
int cCounterIncBoth() {return vtCountersIncBoth(1);};
int aCounterInc() {return _aCounterInc(cBuffer*4);}


// use faster version of BVH builder
#define GSIZE bvhBlock.leafCount
const uint WGS = 16u;
uint wID(in uint t) {return GSIZE * gl_WorkGroupID.x + t;};
uint mrQ(in uint q) {return GSIZE * WGS * q;};
uint rID(in uint t) {return gl_NumWorkGroups.x * t + gl_WorkGroupID.x;};


#define asize_ _counters[(1-cBuffer)*4]
#define asize_inv_ _counters[cBuffer*4]
#define asize _counters[7]
//#define OSi _counters[8]

#ifdef FIRST_STEP
#define osize asize
#define wsize asize
#else
#define osize _counters[6]
#define wsize _counters[5]
#endif

#define swapIn  _counters[1]
#define swapOut _counters[2]

#include "./submodules/bvh-build-general.glsl" // unified functions
layout ( local_size_x = WORK_SIZE_BND ) in;

// I have no idea for optimize registers
void main() {
#define threadID Local_Idx
#define groupSize gl_WorkGroupSize.x

    // lane-based
    const uint gS = (groupSize >> 1), iT = threadID >> 1; const int sD = int(threadID & 1);

    LGROUP_BARRIER
    [[flatten]] if (threadID < 8) { _counters[threadID] = 0; }
    LGROUP_BARRIER

    // create initial (root) node
    if (threadID == 0) {
#ifdef FIRST_STEP
        bvhNodes[0].meta = ivec4(1, bvhBlock.leafCount, 0, 0), bvhNodes[1].meta = ivec4(0, 0, 0, 0);
        Actives[aCounterInc()] = 1;
#else
        cBuffer = 1, asize_inv_ = aCounter[  cBuffer], asize_ = aCounter[1-cBuffer];
#endif
    }
    
    IFALL (bvhBlock.leafCount <= 0) return;

    // building BVH
    [[dependency_infinite]]
    for (int m=0;m<65536;m++) {
        LGROUP_BARRIER

#ifdef FIRST_STEP
        [[flatten]] IFANY ( asize>=WGS && (m&1)==1 ) break; // limit by few elements
#endif

        // swap buffers emulation
        if (threadID == 0) { 
            cBuffer = 1-cBuffer, asize_inv_ = 0, asize = atomicExchange(asize_, 0), swapIn = int(mrQ(1-cBuffer)), swapOut = int(mrQ(cBuffer));//, OSi = int(gS*(m==0?gl_NumWorkGroups.x:1));
#ifndef FIRST_STEP
            osize = m==0?asize:int(wID(asize));
#endif
        }
        
        LGROUP_BARRIER
        
        // split nodes
        const int dT = int(m==0?rID(iT):wID(iT)), OSi = int(gS*(m==0?gl_NumWorkGroups.x:1));
        [[flatten]] IFANY (asize <= 0) break; // check-out limits
        [[unroll, dependency_length(4)]]
        for (int fT=0;fT<asize;fT+=OSi) { SB_BARRIER
            const int iWD = fT+dT;
            [[flatten]] IFALL (iWD >= osize) break;
            [[flatten]] if (iWD < osize) {
                const int fID = iWD < osize ? Actives[swapIn+iWD]-1 : -1;
                [[flatten]] if (sD == 0 && iWD < osize) Actives[swapIn+iWD] = 0;
                [[flatten]] if (fID >= 0) { splitNode(fID|sD); } // split sibling nodes
            }
        }
    }

    LGROUP_BARRIER

#ifdef FIRST_STEP
    if (threadID == 0) { aCounter[cBuffer] = asize_inv_, aCounter[1-cBuffer] = asize_; }
#endif
}
